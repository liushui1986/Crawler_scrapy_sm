{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50fdfa4-c932-4a12-846a-91a922d55526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b380d69c-3fae-4252-9673-0d55ef01536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and parse robots.txt\n",
    "def parse_robots_txt(base_url):\n",
    "    robots_url = urljoin(base_url, '/robots.txt')\n",
    "    response = requests.get(robots_url)\n",
    "    crawl_delay = {}\n",
    "    disallowed_paths = {}\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.split('\\n')\n",
    "        user_agent = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('User-agent:'):\n",
    "                user_agent = line.split(':', 1)[1].strip().lower()\n",
    "                if user_agent not in disallowed_paths:\n",
    "                    disallowed_paths[user_agent] = []\n",
    "            elif user_agent and line.startswith('Disallow:'):\n",
    "                path = line.split(':', 1)[1].strip()\n",
    "                if path:\n",
    "                    disallowed_paths[user_agent].append(path)\n",
    "            elif user_agent and line.startswith('Crawl-delay:'):\n",
    "                delay = int(line.split(':', 1)[1].strip())\n",
    "                crawl_delay[user_agent] = delay\n",
    "    \n",
    "    return crawl_delay, disallowed_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1539d45d-c955-42b0-ac3f-d65eba35a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if URL is allowed to be crawled\n",
    "def is_url_allowed(url, user_agent, disallowed_paths):\n",
    "    parsed_url = urlparse(url)\n",
    "    for agent, paths in disallowed_paths.items():\n",
    "        if agent == '*' or user_agent in agent:\n",
    "            for path in paths:\n",
    "                if parsed_url.path.startswith(path):\n",
    "                    return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8754ceac-813a-45d1-b730-ce6ea54f37dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and parse a page\n",
    "def fetch_page(url, user_agent, disallowed_paths):\n",
    "    if not is_url_allowed(url, user_agent, disallowed_paths):\n",
    "        print(f\"Access to {url} is disallowed by robots.txt\")\n",
    "        return None\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb88acd-a408-4857-b2d3-bd29276ed67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main crawling function\n",
    "def crawl_website(base_url, user_agent='my-crawler'):\n",
    "    crawl_delay, disallowed_paths = parse_robots_txt(base_url)\n",
    "    delay = crawl_delay.get(user_agent, crawl_delay.get('*', 0))\n",
    "\n",
    "    urls_to_crawl = [base_url]\n",
    "    crawled_urls = set()\n",
    "    found_urls = {\"title\": [], \"links\": []}\n",
    "\n",
    "    while urls_to_crawl:\n",
    "        url = urls_to_crawl.pop(0)\n",
    "        if url in crawled_urls:\n",
    "            continue\n",
    "        \n",
    "        page_content = fetch_page(url, user_agent, disallowed_paths)\n",
    "        if page_content:\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            if soup.title:\n",
    "                found_urls[\"title\"].append(soup.title.string)\n",
    "            \n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(base_url, link['href'])\n",
    "                if base_url in absolute_url and absolute_url not in crawled_urls:\n",
    "                    urls_to_crawl.append(absolute_url)\n",
    "                    found_urls[\"links\"].append(absolute_url)\n",
    "        \n",
    "        crawled_urls.add(url)\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return found_urls['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd0c261-6463-46cb-802c-7d92d1ae4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://labs.utsouthwestern.edu/ansir-lab'\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "df_new['links'] = crawl_website(base_url, user_agent='my-crawler')\n",
    "df_new.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e34c2ae7-64ed-4e82-bc7a-3d582b886bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('web_crawler_labweb_test_ShuaipengMa.csv')\n",
    "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "df = pd.concat([df, df_new], ignore_index=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "#df.reset_index(inplace=True)\n",
    "df.to_csv('web_crawler_labweb_test_ShuaipengMa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db800b-e86a-4c0c-9fbd-a3f15eaa691c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
